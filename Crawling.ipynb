{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1KjaTF_v2j8S-hDZglvcThr-ZpdTF_7j-",
      "authorship_tag": "ABX9TyORMU0ptVZuQoKhlI7rkcxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/combined/blob/main/Crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import pandas as pd\n",
        "\n",
        "# BART 모델 및 토크나이저 초기화\n",
        "summarizer_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "summarizer_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n"
      ],
      "metadata": {
        "id": "G0jPeIzRYh2q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 모델 초기화\n",
        "def load_translation_models():\n",
        "    \"\"\"\n",
        "    NLLB 모델과 토크나이저를 초기화합니다.\n",
        "    \"\"\"\n",
        "    languages = ['fra', 'deu', 'rus', 'jpn']  # 프랑스어, 독일어, 러시아어, 일본어\n",
        "    models = {}\n",
        "    base_model = \"facebook/nllb-200-3.3B\"  # NLLB 대형 모델 (3.3B 버전)\n",
        "\n",
        "    # NLLB 모델과 토크나이저 로드\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
        "\n",
        "    # 각 언어 쌍 초기화\n",
        "    for lang in languages:\n",
        "        models[f\"eng_{lang}\"] = {\"model\": model, \"tokenizer\": tokenizer, \"target_lang\": lang}\n",
        "        models[f\"{lang}_kor\"] = {\"model\": model, \"tokenizer\": tokenizer, \"target_lang\": \"kor\"}\n",
        "    return models\n",
        "translation_models = load_translation_models()\n"
      ],
      "metadata": {
        "id": "szCE7PFRfW8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyw3NxanSgUs",
        "outputId": "f61ed7ce-32d6-40b2-af1e-a5a3205d0527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing URL: https://www.bbc.com/news/articles/cd6vpy8e6jvo\n",
            "Processing URL: https://www.bbc.com/news/articles/cly20zz51j7o\n",
            "Processing URL: https://www.bbc.com/future/article/20241115-how-robotaxis-are-trying-to-win-passengers-trust\n",
            "Processing URL: https://www.bbc.com/news/articles/c9wrqg4vd2qo\n",
            "Processing URL: https://www.bbc.com/news/articles/c5yxv41q235o\n",
            "Processing URL: https://www.bbc.com/news/articles/c30p16gn3pvo\n",
            "Processing URL: https://www.bbc.com/news/articles/cdenplz5j89o\n",
            "Processing URL: https://www.bbc.com/future/article/20230306-just-how-loud-is-a-rocket-launch\n",
            "Processing URL: https://www.bbc.com/future/article/20241111-stressed-writing-down-a-to-do-list-might-help\n",
            "Processing URL: https://www.bbc.com/future/article/20201028-the-benefits-of-coffee-is-coffee-good-for-health\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def detect_site(url):\n",
        "    \"\"\"\n",
        "    URL을 분석하여 사이트를 감지합니다.\n",
        "    \"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    domain = parsed_url.netloc\n",
        "\n",
        "    if \"bbc\" in domain:\n",
        "        return \"bbc\"\n",
        "    elif \"cnn\" in domain:\n",
        "        return \"cnn\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_filtered_main_content(url):\n",
        "    \"\"\"\n",
        "    URL에서 원문(article)을 크롤링합니다.\n",
        "    \"\"\"\n",
        "    content = \"\"\n",
        "\n",
        "    # HTML 소스 가져오기\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        return f\"Failed to fetch the webpage! URL: {url}\"\n",
        "\n",
        "    # 인코딩 설정\n",
        "    response.encoding = response.apparent_encoding\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # 사이트 감지\n",
        "    site = detect_site(url)\n",
        "    if site == \"bbc\":\n",
        "        text_blocks = soup.select('div[data-component=\"text-block\"]')\n",
        "        for block in text_blocks:\n",
        "            content += block.get_text(strip=True) + \"\\n\\n\"\n",
        "    elif site == \"cnn\":\n",
        "        paragraphs = soup.select(\"p.paragraph.inline-placeholder.vossi-paragraph\")\n",
        "        for paragraph in paragraphs:\n",
        "            content += paragraph.get_text(strip=True) + \"\\n\\n\"\n",
        "    else:\n",
        "        return None  # 지원되지 않는 사이트\n",
        "\n",
        "    # 텍스트 정리\n",
        "    content = (\n",
        "        content.replace(\"\\s\\s+\", \" \")\n",
        "        .replace(\". \", \".\\n\\n\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    return content or None\n",
        "\n",
        "def split_text_with_last_sentence_overlap(text, target_chunk_length=1024):\n",
        "    \"\"\"\n",
        "    긴 텍스트를 지정된 길이로 문장 단위로 분할하며, 마지막 문장을 중복 포함시킵니다.\n",
        "    \"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk.split()) + len(sentence.split()) <= target_chunk_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            # 현재 청크를 저장하고 다음 청크로 이동, 마지막 문장 포함\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def summarize_long_text(article_text, target_chunk_length=1024):\n",
        "    \"\"\"\n",
        "    긴 텍스트를 분할하여 각 청크를 요약하고 결합합니다.\n",
        "\n",
        "    Args:\n",
        "        article_text (str): 원문 텍스트.\n",
        "        target_chunk_length (int): 각 청크의 최대 토큰 수.\n",
        "\n",
        "    Returns:\n",
        "        str: 전체 텍스트 요약 결과.\n",
        "    \"\"\"\n",
        "    # 텍스트 분할\n",
        "    chunks = split_text_with_last_sentence_overlap(article_text, target_chunk_length)\n",
        "    summaries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        try:\n",
        "            # 텍스트 토크나이징 및 모델 입력 생성\n",
        "            inputs = summarizer_tokenizer(chunk, max_length=target_chunk_length, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "            # 요약 생성\n",
        "            summary_ids = summarizer_model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_length=100,   # 출력 요약 최대 길이\n",
        "                min_length=50,    # 출력 요약 최소 길이\n",
        "                length_penalty=2.0,\n",
        "                num_beams=2,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            # 요약 디코딩\n",
        "            summary = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "            summaries.append(summary)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk: {chunk[:100]}... | Error: {e}\")\n",
        "            summaries.append(\"Summarization failed for this chunk.\")\n",
        "\n",
        "    # 요약된 청크 결합\n",
        "    return \" \".join(summaries)\n",
        "\n",
        "def process_urls_from_file(file_path, output_path):\n",
        "    \"\"\"\n",
        "    URL이 담긴 파일을 처리하여 요약 데이터를 생성하고 저장합니다.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # TXT 파일에서 URL 읽기\n",
        "    with open(file_path, 'r') as file:\n",
        "        urls = file.read().split(',')\n",
        "\n",
        "    for url in urls:\n",
        "        url = url.strip()  # 공백 제거\n",
        "        if not url:\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing URL: {url}\")\n",
        "        article = get_filtered_main_content(url)\n",
        "        if not article:\n",
        "            print(f\"Skipping URL (no article found): {url}\")\n",
        "            continue\n",
        "\n",
        "        # 요약 생성\n",
        "        highlight = summarize_long_text(article)\n",
        "\n",
        "        # 크롤링 결과 저장\n",
        "        results.append({\n",
        "            \"url\": url,\n",
        "            \"article\": article,\n",
        "            \"highlight\": highlight\n",
        "        })\n",
        "\n",
        "    # 결과를 DataFrame으로 변환\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # 데이터 저장 (CSV 형식으로 저장)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Results saved to {output_path}\")\n",
        "\n",
        "# 실행 예제\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/MyDrive/summarizer/data/test_summary.txt\"  # URL이 담긴 TXT 파일 경로\n",
        "    output_file = \"/content/drive/MyDrive/summarizer/data/crawled_summary.csv\"  # 결과 CSV 파일 경로\n",
        "\n",
        "    process_urls_from_file(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def translate_text(text, source_lang, target_lang, model_key):\n",
        "    \"\"\"\n",
        "    텍스트를 지정된 언어로 번역합니다.\n",
        "    \"\"\"\n",
        "    if model_key not in translation_models:\n",
        "        raise ValueError(f\"No translation model for {source_lang} -> {target_lang}\")\n",
        "\n",
        "    model_info = translation_models[model_key]\n",
        "    tokenizer = model_info[\"tokenizer\"]\n",
        "    model = model_info[\"model\"]\n",
        "\n",
        "    # 번역 수행\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs[\"forced_bos_token_id\"] = tokenizer.lang_code_to_id[model_info[\"target_lang\"]]  # 대상 언어 설정\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=512, num_beams=4, early_stopping=True)\n",
        "    translated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return translated_text\n",
        "\n",
        "def create_translation_dataset(summary_df, output_file):\n",
        "    \"\"\"\n",
        "    요약 데이터를 다양한 언어로 번역 후 다시 한국어로 역번역한 데이터셋을 생성합니다.\n",
        "\n",
        "    Args:\n",
        "        summary_df (pd.DataFrame): 요약 데이터가 포함된 데이터프레임. (columns: \"url\", \"article\", \"highlight\")\n",
        "        output_file (str): 결과 CSV 파일 경로.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for _, row in summary_df.iterrows():\n",
        "        original_summary = row[\"highlight\"]  # 영어 원문 요약\n",
        "        translations = {\"en\": original_summary}  # 원문은 'en' 열에 추가\n",
        "\n",
        "        for lang in ['fra', 'deu', 'rus', 'jpn']:  # 프랑스어, 독일어, 러시아어, 일본어\n",
        "            try:\n",
        "                # 영어 -> 중간 언어\n",
        "                intermediate_translation = translate_text(\n",
        "                    original_summary, \"eng\", lang, f\"eng_{lang}\"\n",
        "                )\n",
        "                # 중간 언어 -> 한국어\n",
        "                back_translation = translate_text(\n",
        "                    intermediate_translation, lang, \"kor\", f\"{lang}_kor\"\n",
        "                )\n",
        "                translations[\"ko\"] = back_translation  # 한국어 역번역 결과 추가\n",
        "            except Exception as e:\n",
        "                print(f\"Translation failed for {lang}: {e}\")\n",
        "                translations[\"ko\"] = \"Translation failed\"\n",
        "\n",
        "            # 'ko'와 'en' 열만 유지\n",
        "            results.append({\n",
        "                \"ko\": translations[\"ko\"],\n",
        "                \"en\": translations[\"en\"]\n",
        "            })\n",
        "\n",
        "    # 결과를 데이터프레임으로 변환 후 저장\n",
        "    translation_df = pd.DataFrame(results)\n",
        "    translation_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"Translated dataset saved to {output_file}\")\n",
        "\n",
        "# 실행 예제\n",
        "if __name__ == \"__main__\":\n",
        "    # 요약 데이터 읽기\n",
        "    summary_data_file = \"/content/drive/MyDrive/summarizer/data/crawled_summary.csv\"\n",
        "    summary_df = pd.read_csv(summary_data_file)\n",
        "\n",
        "    # 번역 데이터셋 생성\n",
        "    output_translation_file = \"/content/drive/MyDrive/summarizer/data/translated_summary.csv\"\n",
        "    create_translation_dataset(summary_df, output_translation_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "WLVuH45EerYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRFWd57Ber1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}